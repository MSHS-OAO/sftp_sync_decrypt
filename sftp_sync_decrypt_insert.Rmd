---
title: "SFTP_Sync_Decrypt_Insert"
output: html_document
date: "`r Sys.time()`"
---

```{css, echo = FALSE}
caption {
      color: red;
      font-weight: bold
      font-size: 13;
    }
```

```{r libraries constants, include=F}
library(dplyr)
library(tidyr)
library(glue)
library(readxl)
library(writexl)
library(kableExtra)
library(odbc)
library(DBI)
library(dbplyr)
library(doParallel)

dev_dsn <- "OAO Cloud DB Staging"
prod_dsn <- "OAO Cloud DB Production"

# function to convert each record of df to insert statement
get_values <- function(source_table, destination_schema, destination_table) {
  
  PARTNER                   <- source_table[1]
  HOME_FACILITY             <- source_table[2]
  HOME_DEPARTMENT           <- source_table[3]
  WORKED_FACILITY           <- source_table[4]
  WORKED_DEPARTMENT         <- source_table[5]
  START_DATE                <- source_table[6]
  END_DATE                  <- source_table[7]
  EMPLOYEE_ID               <- source_table[8]
  EMPLOYEE_NAME             <- source_table[9]
  APPROVED_HOURS            <- source_table[10]
  POSITION_CODE             <- source_table[11]
  JOBCODE                   <- source_table[12]
  PAYCODE                   <- source_table[13]
  WD_HOURS                  <- source_table[14]
  WD_EXPENSE                <- source_table[15]
  HOME_DEPARTMENT_NAME      <- source_table[16]
  WORKED_DEPARTMENT_NAME    <- source_table[17]
  POSITION_CODE_DESCRIPTION <- source_table[18]
  LOCATION_DESCRIPTION      <- source_table[19]
  WD_COFT                   <- source_table[20]
  WD_ACCOUNT                <- source_table[21]
  WD_LOCATION               <- source_table[22]
  WD_DEPARTMENT             <- source_table[23]
  WD_FUND_NUMBER            <- source_table[24]
  HD_COFT                   <- source_table[25]
  HD_LOCATION               <- source_table[26]
  HD_DEPARTMENT             <- source_table[27]
  WD_COA                    <- source_table[28]
  HD_COA                    <- source_table[29]
  PAYROLL_NAME              <- source_table[30]
  REVERSE_MAP_WORKED        <- source_table[31]
  REVERSE_MAP_HOME          <- source_table[32]
  FILE_NAME                 <- source_table[33]
  
  values <- glue("INTO {glue::double_quote(destination_schema)}.{glue::double_quote(destination_table)}
                 (PARTNER, HOME_FACILITY, HOME_DEPARTMENT, WORKED_FACILITY, 
                 WORKED_DEPARTMENT, START_DATE, END_DATE, EMPLOYEE_ID, 
                 EMPLOYEE_NAME, APPROVED_HOURS, POSITION_CODE, JOBCODE, PAYCODE,
                 WD_HOURS, WD_EXPENSE, HOME_DEPARTMENT_NAME, 
                 WORKED_DEPARTMENT_NAME, POSITION_CODE_DESCRIPTION, 
                 LOCATION_DESCRIPTION, WD_COFT, WD_ACCOUNT, WD_LOCATION, 
                 WD_DEPARTMENT, WD_FUND_NUMBER, HD_COFT, HD_LOCATION, 
                 HD_DEPARTMENT, WD_COA, HD_COA, PAYROLL_NAME, 
                 REVERSE_MAP_WORKED, REVERSE_MAP_HOME, FILE_NAME)
                 VALUES ('{PARTNER}', '{HOME_FACILITY}', '{HOME_DEPARTMENT}',
                 '{WORKED_FACILITY}', '{WORKED_DEPARTMENT}', 
                 TO_DATE('{START_DATE}', 'yyyy-mm-dd'), 
                 TO_DATE('{END_DATE}', 'yyyy-mm-dd'), 
                 '{EMPLOYEE_ID}', '{EMPLOYEE_NAME}', '{APPROVED_HOURS}', 
                 '{POSITION_CODE}', '{JOBCODE}', '{PAYCODE}', '{WD_HOURS}',
                 '{WD_EXPENSE}', '{HOME_DEPARTMENT_NAME}', 
                 '{WORKED_DEPARTMENT_NAME}', '{POSITION_CODE_DESCRIPTION}',
                 '{LOCATION_DESCRIPTION}', '{WD_COFT}', '{WD_ACCOUNT}',
                 '{WD_LOCATION}', '{WD_DEPARTMENT}', '{WD_FUND_NUMBER}',
                 '{HD_COFT}', '{HD_LOCATION}', '{HD_DEPARTMENT}', '{WD_COA}',
                 '{HD_COA}', '{PAYROLL_NAME}', '{REVERSE_MAP_WORKED}', 
                 '{REVERSE_MAP_HOME}', '{FILE_NAME}')")
  
  return(values)
}

get_values_repdef <- function(source_table, destination_schema, destination_table) {
  
  DEFINITION_CODE         <- source_table[1]
  DEFINITION_NAME         <- source_table[2]
  KEY_VOLUME              <- source_table[3]
  LEGACY_COST_CENTER      <- source_table[4]
  ORACLE_COST_CENTER      <- source_table[5]
  COST_CENTER_DESCRIPTION <- source_table[6]
  SITE                    <- source_table[7]
  CORPORATE_SERVICE_LINE  <- source_table[8]
  CLOSED                  <- source_table[9]
  VP                      <- source_table[10]
  FTE_TREND               <- source_table[11]
  DEPARTMENT_BREAKDOWN    <- source_table[12]
  
  values <- glue("INTO {glue::double_quote(destination_schema)}.{glue::double_quote(destination_table)}
                 (DEFINITION_CODE, DEFINITION_NAME, KEY_VOLUME, 
                  LEGACY_COST_CENTER, ORACLE_COST_CENTER, 
                  COST_CENTER_DESCRIPTION, SITE, CORPORATE_SERVICE_LINE, CLOSED,
                  VP, FTE_TREND, DEPARTMENT_BREAKDOWN)
                 VALUES ('{DEFINITION_CODE}', '{DEFINITION_NAME}', 
                  '{KEY_VOLUME}', '{LEGACY_COST_CENTER}', '{ORACLE_COST_CENTER}', 
                  '{COST_CENTER_DESCRIPTION}', '{SITE}', '{CORPORATE_SERVICE_LINE}',
                 TO_DATE('{CLOSED}', 'yyyy-mm-dd'), '{VP}', '{FTE_TREND}', 
                 '{DEPARTMENT_BREAKDOWN}')")
  
  return(values)
}
```

# SFTP Synchronization
```{r sftp_sync, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# get current MSHQ and BISLR file and date mapping file
mshq_dates <- read_excel(paste0(universal_dir, "Mapping/MSHQ_File_Dates.xlsx"))
bislr_dates <- read_excel(paste0(universal_dir, "Mapping/BISLR_File_Dates.xlsx"))

# get files currently in network directory
mshq_raw_files <- list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Raw/"))
bislr_raw_files <- list.files(paste0(universal_dir, "/Labor//Raw Data/BISLR_sftp_sync/Raw/"))

# run lftp file to sync sftp to network drive
system('lftp -f /data/SFTP/sync_to_network.lftp')

# get new files in network drive
mshq_raw_files_new <- list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Raw/"))
bislr_raw_files_new <- list.files(paste0(universal_dir, "/Labor/Raw Data/BISLR_sftp_sync/Raw/"))

#print new files that were copied over in sync
new_files <- setdiff(c(mshq_raw_files_new, bislr_raw_files_new),
                     c(mshq_raw_files, bislr_raw_files))
if (length(new_files) > 0) {
  cat(cat(paste0(new_files, "\n")), "have been added to the network drive", fill = TRUE)
} else {
  cat("Network drive is already up to date")
}

# add any new files to the mshq dates files
mshq_raw_files_info <- file.info(list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Raw/"), 
                                       full.names = T))
mshq_raw_files_info <- data.frame(RAW = basename(rownames(mshq_raw_files_info)),
                                  RAW_MTIME = mshq_raw_files_info$mtime)
mshq_dates <- mshq_raw_files_info %>%
  left_join(mshq_dates, by = join_by(RAW, RAW_MTIME)) %>%
  select(RAW, DECRYPT, INSERT, RAW_MTIME, START_DATE, END_DATE) %>%
  arrange(RAW_MTIME)
write_xlsx(mshq_dates, paste0(universal_dir, "Mapping/MSHQ_File_Dates.xlsx"))

# add any new files to the bislr dates files
bislr_raw_files_info <- file.info(list.files(paste0(universal_dir, "/Labor/Raw Data/BISLR_sftp_sync/Raw/"), 
                                       full.names = T))
bislr_raw_files_info <- data.frame(RAW = basename(rownames(bislr_raw_files_info)),
                                  RAW_MTIME = bislr_raw_files_info$mtime)
bislr_dates <- bislr_raw_files_info %>%
  left_join(bislr_dates, by = join_by(RAW, RAW_MTIME)) %>%
  select(RAW, DECRYPT, INSERT, RAW_MTIME, START_DATE, END_DATE) %>%
  arrange(RAW_MTIME)
write_xlsx(bislr_dates, paste0(universal_dir, "Mapping/BISLR_File_Dates.xlsx"))

print("sftp_sync successful")
```

# Decryption
#### Import Decryption Key
```{r import_key, echo=FALSE}
system('cat /data/SFTP/MSH_ALLINTERNAL_OB_PRIVATE.asc | gpg --import --no-tty --batch --yes')
print("import_key successful")
```

#### MSHQ Decryption
```{r mshq_decrypt, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# get current MSHQ and file and date mapping file
mshq_dates <- read_excel(paste0(universal_dir, "Mapping/MSHQ_File_Dates.xlsx"))

# list files in the raw and decryption directories
decrypt_files <- list.files(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Decrypt/"))

# list files that need to be decrypted
files_to_decrypt <- mshq_dates %>%
  filter(!(gsub(".out", "", RAW) %in% decrypt_files)) %>%
  select(RAW) %>%
  pull()
  
# decrypt each file in raw folder and not in the decrypted folder
decrypt_function <- lapply(files_to_decrypt, function(x) {
  encrypted <- paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Raw/", x)
  decrypted <- paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Decrypt/", gsub(".out", "", x))
  system(glue("gpg --output '{decrypted}' --decrypt --pinentry-mode loopback --passphrase-file /data/SFTP/password.txt --batch '{encrypted}'"))
  }
)

# get new files in network drive
decrypt_files_new <- list.files(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Decrypt/"))

# print new files that were decrypted
new_files <- setdiff(decrypt_files_new, decrypt_files)
if (length(new_files) > 0) {
  cat(paste(new_files, "has been decrypted"))
} else {
  cat("No new MSHQ files to decrypt")
}

# add any new files to the mshq dates files
mshq_dates <- mshq_dates %>%
  mutate(DECRYPT = case_when(
    is.na(DECRYPT) ~ gsub(".out", "", RAW),
    TRUE ~ DECRYPT))
write_xlsx(mshq_dates, paste0(universal_dir, "Mapping/MSHQ_File_Dates.xlsx"))

print("mshq_decrypt successful")
```

#### BISLR Decryption
```{r bislr_decrypt, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# get current MSHQ and BISLR file and date mapping file
bislr_dates <- read_excel(paste0(universal_dir, "Mapping/BISLR_File_Dates.xlsx"))

# list files in the raw and decryption directories
decrypt_files <- list.files(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Decrypt/"))

# list files that need to be decrypted
files_to_decrypt <- bislr_dates %>%
  filter(!(gsub(".out", "", RAW) %in% decrypt_files)) %>%
  select(RAW) %>%
  pull()
  
# decrypt each file in raw folder and not in the decrypted folder
decrypt_function <- lapply(files_to_decrypt, function(x) {
  encrypted <- paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Raw/", x)
  decrypted <- paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Decrypt/", gsub(".out", "", x))
  system(glue("gpg --output '{decrypted}' --decrypt --pinentry-mode loopback --passphrase-file /data/SFTP/password.txt --batch '{encrypted}'"))
  }
)

# get new files in network drive
decrypt_files_new <- list.files(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Decrypt/"))

# print new files that were decrypted
new_files <- setdiff(decrypt_files_new, decrypt_files)
if (length(new_files) > 0) {
  cat(paste(new_files, "has been decrypted"))
} else {
  cat("No new BISLR files to decrypt")
}

# add any new files to the mshq dates files
bislr_dates <- bislr_dates %>%
  mutate(DECRYPT = case_when(
    is.na(DECRYPT) ~ gsub(".out", "", RAW),
    TRUE ~ DECRYPT))
write_xlsx(bislr_dates, paste0(universal_dir, "Mapping/BISLR_File_Dates.xlsx"))

print("bislr_decrypt successful")
```

# Insert Creation
#### MSHQ Inserts
```{r mshq_insert, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# col names for Oracle DB
db_columns <- c("PARTNER", "HOME_FACILITY", "HOME_DEPARTMENT", "WORKED_FACILITY", 
                "WORKED_DEPARTMENT", "START_DATE", "END_DATE", "EMPLOYEE_ID",
                "EMPLOYEE_NAME", "APPROVED_HOURS", "POSITION_CODE", "JOBCODE",
                "PAYCODE", "WD_HOURS", "WD_EXPENSE", "HOME_DEPARTMENT_NAME",
                "WORKED_DEPARTMENT_NAME", "POSITION_CODE_DESCRIPTION",
                "LOCATION_DESCRIPTION", "WD_COFT", "WD_ACCOUNT", "WD_LOCATION",
                "WD_DEPARTMENT", "WD_FUND_NUMBER", "HD_COFT", "HD_LOCATION",
                "HD_DEPARTMENT", "WD_COA", "HD_COA", "PAYROLL_NAME",
                "REVERSE_MAP_WORKED", "REVERSE_MAP_HOME", "FILE_NAME")

# read in mshq dates file 
mshq_dates <- read_excel(paste0(universal_dir, "Mapping/MSHQ_File_Dates.xlsx")) %>%
  mutate(INSERT = case_when(
    START_DATE == "IGNORE" ~ "IGNORE",
    is.na(START_DATE) ~ NA,
    TRUE ~ INSERT))
# get list of files that need dates/Ignore
mshq_dates_new <- mshq_dates %>%
  filter(is.na(START_DATE))

# get list of files in insert folder currently
insert_files <- list.files(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Insert/")) 

# update mshq dates for files with newly added START_DATE and END_DATE values
mshq_dates <- mshq_dates %>% 
  arrange(START_DATE) %>% 
  mutate(INSERT = case_when(
    is.na(INSERT) & !is.na(START_DATE) & START_DATE != "IGNORE" ~ paste0(row.names(.), "_", DECRYPT),
    TRUE ~ INSERT)) %>%
  arrange(RAW_MTIME)

# get list of files that now have date values but dont have an insert file
new_inserts <- setdiff(mshq_dates %>% 
                         filter(!is.na(INSERT) & START_DATE != "IGNORE") %>% 
                         select(INSERT) %>% 
                         pull(),
                       insert_files)
# list of files that need an insert file created
create_inserts <- mshq_dates %>%
  filter(INSERT %in% new_inserts) %>%
  select(DECRYPT) %>%
  pull()

# create a new insert file for all decrypted files that need insert files
inserts <- lapply(create_inserts, function(x) {
  data <-  read.csv(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Decrypt/", x),
                    sep = "~", header = T, stringsAsFactors = F,
                    colClasses = rep("character", 32), strip.white = TRUE) %>%
    mutate(FILENAME = mshq_dates %>%
             filter(DECRYPT == x) %>%
             select(INSERT) %>%
             pull()) %>%
    mutate(Start.Date = format(as.Date(Start.Date, format = "%m/%d/%Y"), "%Y-%m-%d"),
           End.Date = format(as.Date(End.Date, format = "%m/%d/%Y"), "%Y-%m-%d"),
           Job.Code = case_when(
             Job.Code == "" ~ "UNKNOWN",
             TRUE ~ Job.Code)) %>%
    filter(Start.Date >= as.Date(pull(filter(mshq_dates, DECRYPT == x) %>%
                                         select(START_DATE)), format = "%Y-%m-%d"),
           End.Date <= as.Date(pull(filter(mshq_dates, DECRYPT == x) %>%
                                      select(END_DATE)), format = "%Y-%m-%d"))
})
# apply column names of oracle db to all new insert files
inserts <- lapply(inserts, setNames, db_columns)
names(inserts) <- create_inserts

insert_function <- lapply(names(inserts), function(x) {
  write.table(inserts[[x]], paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Insert/",
                                   mshq_dates %>% 
                                     filter(DECRYPT == x) %>%
                                     select(INSERT) %>%
                                     pull()),
              sep = "~", row.names = FALSE)
})

# save updated mshq dates file
write_xlsx(mshq_dates, paste0(universal_dir, "Mapping/MSHQ_File_Dates.xlsx"))

# list new insert files if there are any
if (length(new_inserts) > 0) {
  cat(cat(paste0(new_inserts, "\n")), "has been added to the Insert folder", fill = TRUE)
} else {
  cat("Insert files are already up to date")
}

# list files that need START_DATE and END_DATE values
if (nrow(mshq_dates_new) > 0) {
  kable(mshq_dates_new, 
        caption = "The following files need START_DATE and END_DATE values in MSHQ_File_Dates.xlsx:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
} else {
  cat("MSHQ_File_Dates.xlsx is up to date")
}

print("mshq_insert successful")
```

#### BISLR Inserts
```{r bislr_insert, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# col names for Oracle DB
db_columns <- c("PARTNER", "HOME_FACILITY", "HOME_DEPARTMENT", "WORKED_FACILITY", 
                "WORKED_DEPARTMENT", "START_DATE", "END_DATE", "EMPLOYEE_ID",
                "EMPLOYEE_NAME", "APPROVED_HOURS", "POSITION_CODE", "JOBCODE",
                "PAYCODE", "WD_HOURS", "WD_EXPENSE", "HOME_DEPARTMENT_NAME",
                "WORKED_DEPARTMENT_NAME", "POSITION_CODE_DESCRIPTION",
                "LOCATION_DESCRIPTION", "WD_COFT", "WD_ACCOUNT", "WD_LOCATION",
                "WD_DEPARTMENT", "WD_FUND_NUMBER", "HD_COFT", "HD_LOCATION",
                "HD_DEPARTMENT", "WD_COA", "HD_COA", "PAYROLL_NAME",
                "REVERSE_MAP_WORKED", "REVERSE_MAP_HOME", "FILE_NAME")

#Names of the weekly paycyle names in the payroll name column in data files
weekly_pc <- c("WEST WEEKLY", "BIB WEEKLY")

# read in bislr dates file 
bislr_dates <- read_excel(paste0(universal_dir, "Mapping/BISLR_File_Dates.xlsx")) %>%
  mutate(INSERT = case_when(
    START_DATE == "IGNORE" ~ "IGNORE",
    is.na(START_DATE) ~ NA,
    TRUE ~ INSERT))

# get list of files that need dates/Ignore
bislr_dates_new <- bislr_dates %>%
  filter(is.na(START_DATE))

# get list of files in insert folder currently
insert_files <- list.files(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Insert/")) 

# update mshq dates for files with newly added START_DATE and END_DATE values
bislr_dates <- bislr_dates %>% 
  arrange(START_DATE) %>% 
  mutate(INSERT = case_when(
    is.na(INSERT) & !is.na(START_DATE) & START_DATE != "IGNORE" ~ paste0(row.names(.), "_", DECRYPT),
    TRUE ~ INSERT)) %>%
  arrange(RAW_MTIME)

# get list of files that now have date values but dont have an insert file
new_inserts <- setdiff(bislr_dates %>% 
                         filter(!is.na(INSERT) & START_DATE != "IGNORE") %>% 
                         select(INSERT) %>% 
                         pull(),
                       insert_files)
# list of files that need an insert file created
create_inserts <- bislr_dates %>%
  filter(INSERT %in% new_inserts) %>%
  select(DECRYPT) %>%
  pull()

# create list of start and end dates for all insert files
delete_weekly <- lapply(create_inserts, function(x) {
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Decrypt/", x),
                    sep = "~", header = T, stringsAsFactors = F,
                    colClasses = rep("character", 32), strip.white = TRUE) %>%
     mutate(Start.Date = format(as.Date(Start.Date, format = "%m/%d/%Y"), "%Y-%m-%d"),
           End.Date = format(as.Date(End.Date, format = "%m/%d/%Y"), "%Y-%m-%d")) %>%
    filter(Start.Date >= as.Date(pull(filter(bislr_dates, DECRYPT == x) %>%
                                         select(START_DATE)), format = "%Y-%m-%d"),
           End.Date <= as.Date(pull(filter(bislr_dates, DECRYPT == x) %>%
                                      select(END_DATE)), format = "%Y-%m-%d")) %>%
    arrange(Start.Date, End.Date) %>%
    mutate(Start_End = paste0(Start.Date, "_", End.Date)) %>%
    select(Start.Date, End.Date, Start_End) %>%
    distinct() %>%
    filter(Start.Date == max(Start.Date))
})
# assign create_inserts as the name for the delete weekly list elements
names(delete_weekly) <- create_inserts

# create a new insert file for all decrypted files that need insert files
inserts <- lapply(create_inserts, function(x) {
  data <-  read.csv(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Decrypt/", x),
                    sep = "~", header = T, stringsAsFactors = F,
                    colClasses = rep("character", 32), strip.white = TRUE) %>%
    mutate(FILENAME = bislr_dates %>%
             filter(DECRYPT == x) %>%
             select(INSERT) %>%
             pull()) %>%
    mutate(Start.Date = format(as.Date(Start.Date, format = "%m/%d/%Y"), "%Y-%m-%d"),
           End.Date = format(as.Date(End.Date, format = "%m/%d/%Y"), "%Y-%m-%d"),
           Job.Code = case_when(
             Job.Code == "" ~ "UNKNOWN",
             TRUE ~ Job.Code)) %>%
    filter(Start.Date >= as.Date(pull(filter(bislr_dates, DECRYPT == x) %>%
                                         select(START_DATE)), format = "%Y-%m-%d"),
           End.Date <= as.Date(pull(filter(bislr_dates, DECRYPT == x) %>%
                                      select(END_DATE)), format = "%Y-%m-%d")) %>%
    mutate(Start_End = paste0(Start.Date, "_", End.Date)) %>%
    filter(Start_End != delete_weekly[[x]][,"Start_End"]) %>%
    select(-Start_End)
})
# apply column names of oracle db to all new insert files
inserts <- lapply(inserts, setNames, db_columns)
names(inserts) <- create_inserts

insert_function <- lapply(names(inserts), function(x) {
  write.table(inserts[[x]], paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Insert/",
                        bislr_dates %>% 
                          filter(DECRYPT == x) %>%
                          select(INSERT) %>%
                          pull()),
              sep = "~", row.names = FALSE)
})

# save updated mshq dates file
write_xlsx(bislr_dates, paste0(universal_dir, "Mapping/BISLR_File_Dates.xlsx"))

# list new insert files if there are any
if (length(new_inserts) > 0) {
  cat(cat(paste0(new_inserts, "\n")), "has been added to the Insert folder", fill = TRUE)
} else {
  cat("Insert files are already up to date")
}

# list files that need START_DATE and END_DATE values
if (nrow(bislr_dates_new) > 0) {
  kable(bislr_dates_new, 
        caption = "The following files need START_DATE and END_DATE values in BISLR_File_Dates.xlsx:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
} else {
  cat("BISLR_File_Dates.xlsx is up to date")
}

print("bislr_insert successful")
```

# Quality Check
#### Job Code Check
```{r jobcode check, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# establish connection to OAO oracle cloud db
con_dev <- dbConnect(odbc(), dev_dsn)

# get set of unique files in MSHQ database
mshq_db_files <- tbl(con_dev, "LPM_DATA_MSHQ_ORACLE") %>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull

# get set of unique files in BISLR database
bislr_db_files <- tbl(con_dev, "LPM_DATA_BISLR_ORACLE") %>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull()

# get new files in network drive
mshq_inserts <- list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Insert/"))
bislr_inserts <- list.files(paste0(universal_dir, "/Labor/Raw Data/BISLR_sftp_sync/Insert/"))

# get set of files to check the mapping files with
mshq_new <- setdiff(mshq_inserts, mshq_db_files)
bislr_new <- setdiff(bislr_inserts, bislr_db_files)

# get all jobcodes in mapping file within database
jobcodes <- tbl(con_dev, "LPM_MAPPING_JOBCODE") %>%
  select(JOBCODE, PAYROLL) %>%
  collect()

# read all insert files that need jobcode check and bind them
mshq_check <- lapply(mshq_new, function(x) {
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Insert/", x),
                    sep = "~", header = T, stringsAsFactors = F,
                    colClasses = rep("character", 33), strip.white = TRUE)
})
mshq_check <- do.call("rbind", mshq_check)

bislr_check <- lapply(bislr_new, function(x) {
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Insert/", x),
                    sep = "~", header = T, stringsAsFactors = F,
                    colClasses = rep("character", 33), strip.white = TRUE)
})
bislr_check <- do.call("rbind", bislr_check)

# check jobcodes in new insert files with jobcode mapping file
if (is.null(mshq_check)) {
  cat("MSHQ Jobcodes are up to date in LPM_MAPPING_JOBCODE")
} else if (length(setdiff((mshq_check %>% 
                    select(JOBCODE) %>% pull()),
                   (jobcodes %>% 
                    filter(PAYROLL == 'MSHQ') %>% select(JOBCODE) %>% pull()))) == 0){
  cat("MSHQ Jobcodes are up to date in LPM_MAPPING_JOBCODE")
} else {
  # create data frame for new jobcodes at MSHQ
  new_mshq_jobcodes <- mshq_check %>%
    filter(!(JOBCODE %in% (jobcodes %>% 
                             filter(PAYROLL == 'MSHQ') %>% 
                             select(JOBCODE) %>% pull))) %>%
    mutate(PAYROLL = "MSHQ",
           PROVIDER = "",
           JOBCODE_PREMIER = "",
           JOBCODE_PREMIER_DESCRIPTION = "") %>%
    rename(JOBCODE_DESCRIPTION = POSITION_CODE_DESCRIPTION) %>%
    select(JOBCODE, PAYROLL, JOBCODE_DESCRIPTION, PROVIDER, JOBCODE_PREMIER, JOBCODE_PREMIER_DESCRIPTION) %>%
    distinct()
  
  # save df with new MSHQ jobcodes
  write_xlsx(new_mshq_jobcodes, paste0(universal_dir, 
                                       "Mapping/sftp_sync_decrypt_insert/JobCode/MSHQ/new_mshq_jobcodes_", 
                                       Sys.Date(), ".xlsx"))
  
  # print kable of new MSHQ jobcodes
  kable(new_mshq_jobcodes, 
        caption = "The following MSHQ jobcodes need to be added to MAPING_JOBCODE:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
}


# check jobcodes in new insert files with jobcode mapping file
if (is.null(bislr_check)) {
  cat("BISLR Jobcodes are up to date in LPM_MAPPING_JOBCODE")
} else if (length(setdiff((bislr_check %>% 
                    select(JOBCODE) %>% pull()),
                   (jobcodes %>% 
                    filter(PAYROLL == 'BISLR') %>% select(JOBCODE) %>% pull()))) == 0){
  cat("BISLR Jobcodes are up to date in LPM_MAPPING_JOBCODE")
} else {
  # create data frame for new jobcodes at BISLR
  new_bislr_jobcodes <- bislr_check %>%
    filter(!(JOBCODE %in% (jobcodes %>% 
                             filter(PAYROLL == 'BISLR') %>% 
                             select(JOBCODE) %>% pull))) %>%
    mutate(PAYROLL = "BISLR",
           PROVIDER = "",
           JOBCODE_PREMIER = "",
           JOBCODE_PREMIER_DESCRIPTION = "") %>%
    rename(JOBCODE_DESCRIPTION = POSITION_CODE_DESCRIPTION) %>%
    select(JOBCODE, PAYROLL, JOBCODE_DESCRIPTION, PROVIDER, JOBCODE_PREMIER, JOBCODE_PREMIER_DESCRIPTION) %>%
    distinct()
  
  # save df with new BISLR jobcodes
  write_xlsx(new_bislr_jobcodes, paste0(universal_dir, 
                                       "Mapping/sftp_sync_decrypt_insert/JobCode/BISLR/new_bislr_jobcodes_", 
                                       Sys.Date(), ".xlsx"))
  
  # print kable of new BISLR jobcodes
  kable(new_bislr_jobcodes, 
        caption = "The following BISLR jobcodes need to be added to MAPING_JOBCODE:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
}

print("Jobcode check successful")
```

#### Pay Code Check
```{r paycode check , echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# establish connection to OAO oracle cloud db
con_dev <- dbConnect(odbc(), dev_dsn)

# Unique MSHQ files in database
mshq_db_files <- tbl(con_dev, "LPM_DATA_MSHQ_ORACLE")%>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull()
  
# Unique BISLR files in database
bislr_db_files <- tbl(con_dev, "LPM_DATA_BISLR_ORACLE")%>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull()

# get list of files in insert folder currently
mshq_insert_files <- list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Insert/"))
bislr_insert_files <- list.files(paste0(universal_dir, "/Labor//Raw Data/BISLR_sftp_sync/Insert/"))

# Get new files
mshq_new <- setdiff(mshq_insert_files, mshq_db_files)
bislr_new <- setdiff(bislr_insert_files, bislr_db_files)

# Read mapping pay code
mapping_paycode <- tbl(con_dev, "LPM_MAPPING_PAYCODE") %>%
                   select(PAYCODE_RAW) %>%
                   distinct() %>%
                   collect() %>%
                   pull()

# read in new insert files and bind them all
mshq_paycodes <- lapply(mshq_new, function(x){
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Insert/", x), sep = "~", header = T,
                   stringsAsFactors = F,
                   colClasses = rep("character", 33),
                   strip.white = TRUE) 
 })
mshq_paycodes <- do.call("rbind", mshq_paycodes)

bislr_paycodes <- lapply(bislr_new , function(x){
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Insert/", x), sep = "~", header = T,
                   stringsAsFactors = F,
                   colClasses = rep("character", 33),
                   strip.white = TRUE) 
 })
bislr_paycodes <- do.call("rbind", bislr_paycodes)

new_paycodes <- rbind(mshq_paycodes, bislr_paycodes)

# get new paycodes in all new insert files
if (!is.null(new_paycodes)) {
  new_paycodes <- new_paycodes %>%
         select(FILE_NAME, PAYCODE) %>%
    filter(!(PAYCODE %in% mapping_paycode)) %>%
    distinct()
}

if(is.null(new_paycodes)) {
  cat("MSHS Paycodes are up to date in LPM_MAPPING_PAYCODE")
} else if (nrow(new_paycodes) == 0) {
  cat("MSHS Paycodes are up to date in LPM_MAPPING_PAYCODE")
} else {
  # save new paycodes df
  write_xlsx(new_paycodes, paste0(universal_dir, 
                                  "Mapping/sftp_sync_decrypt_insert/PayCode/new_paycodes_", 
                                       Sys.Date(), ".xlsx"))
  # print new paycodes df
  kable(new_paycodes, 
        caption = "The following MSHS paycodes need to be added to MAPING_PAYCODE:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
}

print("Paycode check successful")
```

#### Pay Cycle Check
```{r paycycle check , echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# establish connection to OAO oracle cloud db
con_dev <- dbConnect(odbc(), dev_dsn)

# Unique MSHQ files in database
mshq_db_files <- tbl(con_dev, "LPM_DATA_MSHQ_ORACLE")%>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull()
  
# Unique BISLR files in database
bislr_db_files <- tbl(con_dev, "LPM_DATA_BISLR_ORACLE")%>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull()

# get list of files in insert folder currently
mshq_insert_files <- list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Insert/"))
bislr_insert_files <- list.files(paste0(universal_dir, "/Labor//Raw Data/BISLR_sftp_sync/Insert/"))

# Get new files
mshq_new <- setdiff(mshq_insert_files, mshq_db_files)
bislr_new <- setdiff(bislr_insert_files, bislr_db_files)

# Read mapping paycycle
unique_dates <- tbl(con_dev, "LPM_MAPPING_PAYCYCLE") %>%
                    collect() %>%
                    select(PAYCYCLE_DATE) %>%
                    mutate(PAYCYCLE_DATE = as.Date(PAYCYCLE_DATE)) %>%
                    distinct() %>%
                    pull()

# read new insert files for MSHS and bind them all
mshq_paycycles <- lapply(mshq_new, function(x){
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Insert/", x), sep = "~", header = T,
                   stringsAsFactors = F,
                   colClasses = rep("character", 33),
                   strip.white = TRUE)
})
mshq_paycycles <- do.call("rbind", mshq_paycycles)

bislr_paycycles <- lapply(bislr_new , function(x){
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Insert/", x), sep = "~", header = T,
                   stringsAsFactors = F,
                   colClasses = rep("character", 33),
                   strip.white = TRUE) 
})
bislr_paycycles  <- do.call("rbind", bislr_paycycles)

new_paycycles <- rbind(mshq_paycycles, bislr_paycycles)

# get df of all new paycycles
if (!is.null(new_paycycles)) {
  new_paycycles <- new_paycycles %>%
   mutate(END_DATE= as.Date(END_DATE)) %>%
         select(FILE_NAME, END_DATE) %>%
    filter(!(END_DATE %in% unique_dates)) %>%
    distinct()
}

if (is.null(new_paycycles)) {
  cat("MSHS End Dates are included in LPM_MAPPING_PAYCYCLE")
} else if (nrow(new_paycycles) == 0) {
  cat("MSHS End Dates are included in LPM_MAPPING_PAYCYCLE")
} else {
  # save df of new paycycles
  write_xlsx(new_paycycles, paste0(universal_dir, 
                                  "Mapping/sftp_sync_decrypt_insert/PayCycle/new_paycycles_", 
                                       Sys.Date(), ".xlsx"))
  # print new paycycles
  kable(new_paycycles, 
        caption = "The following MSHS End Dates are not included in MAPING_PAYCYCLES:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
}

print("Paycycle check successful")
```

#### Insert Check
```{r Insert Check, echo=FALSE}
# establish connection to OAO oracle cloud db
con_dev <- dbConnect(odbc(), dev_dsn)

# mshq check for inserts ready to be uploaded to OAO DB
mshq_eligible_insert <- setdiff(mshq_inserts,
          tbl(con_dev, "LPM_DATA_MSHQ_ORACLE") %>%
            select(FILE_NAME) %>%
            distinct() %>%
            collect() %>%
            pull())
if (length(mshq_eligible_insert > 0)) {
  cat(paste0(mshq_eligible_insert, " is ready to be uploaded to the OAO DB"))
} else {
  cat("LPM_DATA_MSHQ_ORACLE is up to date")
}


# bislr check for inserts ready to be uploaded to OAO DB
bislr_eligible_insert <- setdiff(bislr_inserts,
          tbl(con_dev, "LPM_DATA_BISLR_ORACLE") %>%
            select(FILE_NAME) %>%
            distinct() %>%
            collect() %>%
            pull())
if (length(bislr_eligible_insert > 0)) {
  cat(paste0(bislr_eligible_insert, " is ready to be uploaded to the OAO DB"))
} else {
  cat("LPM_DATA_BISLR_ORACLE is up to date")
}
```

# Merge Schemas
### Merge Mapping Tables
```{r Merge Mapping Tables , echo=FALSE}
# merge statement jobcode mapping table
jobcode_merge <- glue(
  "MERGE INTO LPM_MAPPING_JOBCODE destination
   USING OAO_DEVELOPMENT.LPM_MAPPING_JOBCODE source
   ON (destination.PAYROLL = source.PAYROLL AND
       destination.JOBCODE = source.JOBCODE )
   WHEN MATCHED THEN
   UPDATE SET destination.JOBCODE_DESCRIPTION = source.JOBCODE_DESCRIPTION,
              destination.PROVIDER = source.PROVIDER,
              destination.JOBCODE_PREMIER = source.JOBCODE_PREMIER,
              destination.JOBCODE_PREMIER_DESCRIPTION = source.JOBCODE_PREMIER_DESCRIPTION
   WHEN NOT MATCHED THEN
   INSERT (destination.JOBCODE,
           destination.PAYROLL,
           destination.JOBCODE_DESCRIPTION,
           destination.PROVIDER,
           destination.JOBCODE_PREMIER,
           destination.JOBCODE_PREMIER_DESCRIPTION)
   VALUES (source.JOBCODE,
           source.PAYROLL,
           source.JOBCODE_DESCRIPTION,
           source.PROVIDER,
           source.JOBCODE_PREMIER,
           source.JOBCODE_PREMIER_DESCRIPTION);"
)


# merge statement for paycode mapping table
paycode_merge <- glue(
  "MERGE INTO LPM_MAPPING_PAYCODE destination
   USING OAO_DEVELOPMENT.LPM_MAPPING_PAYCODE source
   ON (destination.PAYCODE_RAW = source.PAYCODE_RAW)
   WHEN MATCHED THEN
   UPDATE SET destination.PAYCODE_PREMIER = source.PAYCODE_PREMIER,
              destination.PAYCODE_DESCRIPTION = source.PAYCODE_DESCRIPTION,
              destination.PAYCODE_CATEGORY = source.PAYCODE_CATEGORY,
              destination.INCLUDE_HOURS = source.INCLUDE_HOURS,
              destination.INCLUDE_EXPENSES = source.INCLUDE_EXPENSES,
              destination.WORKED_PAYCODE = source.WORKED_PAYCODE
   WHEN NOT MATCHED THEN
   INSERT (destination.PAYCODE_RAW,
           destination.PAYCODE_PREMIER,
           destination.PAYCODE_DESCRIPTION,
           destination.PAYCODE_CATEGORY,
           destination.INCLUDE_HOURS,
           destination.INCLUDE_EXPENSES,
           destination.WORKED_PAYCODE)
   VALUES (source.PAYCODE_RAW,
           source.PAYCODE_PREMIER,
           source.PAYCODE_DESCRIPTION,
           source.PAYCODE_CATEGORY,
           source.INCLUDE_HOURS,
           source.INCLUDE_EXPENSES,
           source.WORKED_PAYCODE);"
)

# merge statement for paycycle mapping table
paycycle_merge <- glue(
  "MERGE INTO LPM_MAPPING_PAYCYCLE destination
   USING OAO_DEVELOPMENT.LPM_MAPPING_PAYCYCLE source
   ON (destination.PAYCYCLE_DATE = source.PAYCYCLE_DATE)
   WHEN MATCHED THEN
   UPDATE SET destination.PP_START_DATE = source.PP_START_DATE,
              destination.PP_END_DATE = source.PP_END_DATE,
              destination.PREMIER_DISTRIBUTION = source.PREMIER_DISTRIBUTION
   WHEN NOT MATCHED THEN
   INSERT (destination.PP_START_DATE,
           destination.PP_END_DATE,
           destination.PREMIER_DISTRIBUTION)
   VALUES (source.PP_START_DATE,
           source.PP_END_DATE,
           source.PREMIER_DISTRIBUTION);"
) 

# truncate and insert code for LPM_MAPPING_REPDEF
# establish destination table
repdef_table <- "LPM_MAPPING_REPDEF"
repdef_schema <- "OAO_PRODUCTION"

# establish connection to OAO Development
con_prod <- dbConnect(odbc(), prod_dsn)

# truncate query
repdef_truncate <- glue(
  "TRUNCATE TABLE LPM_MAPPING_REPDEF"
)

# get repdef table from source schema
repdef_data <- tbl(con_dev, in_schema("OAO_DEVELOPMENT", "LPM_MAPPING_REPDEF")) %>%
  collect() %>%
  mutate(across(everything(), as.character)) %>%
  mutate(DEFINITION_CODE = replace_na(DEFINITION_CODE, ""),
         DEFINITION_NAME = replace_na(DEFINITION_NAME, ""),
         KEY_VOLUME = replace_na(KEY_VOLUME, ""),
         LEGACY_COST_CENTER = replace_na(LEGACY_COST_CENTER, ""),
         ORACLE_COST_CENTER = replace_na(ORACLE_COST_CENTER, ""),
         COST_CENTER_DESCRIPTION = replace_na(COST_CENTER_DESCRIPTION, ""),
         SITE = replace_na(SITE, ""),
         CORPORATE_SERVICE_LINE = replace_na(CORPORATE_SERVICE_LINE, ""),
         CLOSED = replace_na(CLOSED, ""),
         VP = replace_na(VP, ""),
         FTE_TREND = replace_na(FTE_TREND, ""),
         DEPARTMENT_BREAKDOWN = replace_na(DEPARTMENT_BREAKDOWN, ""),
         DEFINITION_NAME = gsub("\'", "''", DEFINITION_NAME),
         COST_CENTER_DESCRIPTION = gsub("\'", "''", COST_CENTER_DESCRIPTION))

# convert the each record/row of tibble to INTO clause of insert statment
inserts <- 
  lapply(
    lapply(
      lapply(split(repdef_data, 
                   1:nrow(repdef_data)),
             as.list),
      as.character),
    FUN = get_values_repdef, repdef_schema, repdef_table)

# create batches of inserts for insert statements
chunk_length <- 250
split_queries <- split(inserts, ceiling(seq_along(inserts)/chunk_length))

# append each batch of inserts to batch insert list
split_queries_values <- list()
for (i in 1:length(split_queries)) {
  row <- glue_collapse(split_queries[[i]], sep = "\n\n")
  values <- glue('INSERT ALL
                 {row}
                 SELECT 1 from DUAL;')
  split_queries_values <- append(split_queries_values, values)
}

# truncate destination table before insert
dbBegin(con_prod)
tryCatch({
  dbExecute(con_prod, repdef_truncate)
  cat("LPM_MAPPING_REPDEF has been truncated")
  dbCommit(con_prod)
  dbDisconnect(con_prod)
  }, 
  error = function(err){
    dbRollback(con_prod)
    dbDisconnect(con_prod)
    print("Error")
    }
  )
# execute parallel inserts of 250 record chunks
registerDoParallel()
outputPar <- foreach(i = 1:length(split_queries_values), 
                     .packages = c("DBI", "odbc")) %dopar% {
                       con_prod <- dbConnect(odbc(), prod_dsn)
                       tryCatch({
                         dbBegin(con_prod)
                         dbExecute(con_prod, split_queries_values[[i]])
                         dbCommit(con_prod)
                         dbDisconnect(con_prod)
                         },
                         error = function(err){
                           print("error")
                           dbRollback(con_prod)
                           dbDisconnect(con_prod)
                         })
                       }
registerDoSEQ()
cat(paste(repdef_table, "has been updated"))

# execute mapping table merge statements
con_prod <- dbConnect(odbc(), prod_dsn)
dbBegin(con_prod)
# execute statements and if there is an error  with one of them rollback changes
tryCatch({
  dbExecute(con_prod, jobcode_merge)
  cat("LPM_MAPPING_JOBCODE has been merged")
  dbExecute(con_prod, paycode_merge)
  cat("LPM_MAPPING_PAYCODE has been merged")
  dbExecute(con_prod, paycycle_merge)
  cat("LPM_MAPPING_PAYCYCLE has been merged")
  dbCommit(con_prod)
  dbDisconnect(con_prod)
  }, 
  error = function(err){
    dbRollback(con_prod)
    dbDisconnect(con_prod)
    print("Error")
    }
  )
```

### Update MSHQ Data Tables
```{r Update MSHQ Oracle Data Tables , echo=FALSE}
# establish connection to OAO Development
con_prod <- dbConnect(odbc(), prod_dsn)
con_dev <- dbConnect(odbc(), dev_dsn)

# establish destination table
mshq_destination_table <- "LPM_DATA_MSHQ_ORACLE"
mshq_destination_schema <- "OAO_PRODUCTION"

# identify what files are present in dev schema and not production
mshq_inserts_diff <- setdiff(tbl(con_dev, "LPM_DATA_MSHQ_ORACLE") %>%
                          select(FILE_NAME) %>%
                          distinct() %>%
                          collect() %>%
                          pull(),
                        tbl(con_prod, "LPM_DATA_MSHQ_ORACLE") %>%
                          select(FILE_NAME) %>%
                          distinct() %>%
                          collect() %>%
                          pull())

if (length(mshq_inserts_diff) > 0) {
  # prepare the insert df based on files not present in prod
  mshq_insert_df <- tbl(con_dev, "LPM_DATA_MSHQ_ORACLE") %>%
    filter(FILE_NAME %in% mshq_inserts_diff) %>%
    collect() %>%
    mutate(POSITION_CODE = replace_na(POSITION_CODE, ""),
           POSITION_CODE_DESCRIPTION = replace_na(POSITION_CODE_DESCRIPTION, ""),
           LOCATION_DESCRIPTION = replace_na(LOCATION_DESCRIPTION, ""),
           REVERSE_MAP_HOME = replace_na(REVERSE_MAP_HOME, ""),
           REVERSE_MAP_WORKED = replace_na(REVERSE_MAP_WORKED, ""),
           HOME_DEPARTMENT_NAME = gsub("\'", "''", HOME_DEPARTMENT_NAME),
           WORKED_DEPARTMENT_NAME = gsub("\'", "''", WORKED_DEPARTMENT_NAME),
           POSITION_CODE_DESCRIPTION = gsub("\'", "''", POSITION_CODE_DESCRIPTION),
           EMPLOYEE_NAME = gsub("\'", "''", EMPLOYEE_NAME),
           LOCATION_DESCRIPTION = gsub("\'", "''", LOCATION_DESCRIPTION)) %>%
    mutate(across(everything(), as.character))
  
  # convert the each record/row of tibble to INTO clause of insert statment
  inserts <- 
    lapply(
      lapply(
        lapply(split(mshq_insert_df, 
                     1:nrow(mshq_insert_df)),
               as.list),
        as.character),
      FUN = get_values, mshq_destination_schema, mshq_destination_table)
  
  # create batches of inserts for insert statements
  chunk_length <- 250
  split_queries <- split(inserts, ceiling(seq_along(inserts)/chunk_length))
  
  # append each batch of inserts to batch insert list
  split_queries_values <- list()
  for (i in 1:length(split_queries)) {
    row <- glue_collapse(split_queries[[i]], sep = "\n\n")
    values <- glue('INSERT ALL
                 {row}
                 SELECT 1 from DUAL;')
    split_queries_values <- append(split_queries_values, values)
  }
  
  # execute parallel inserts of 250 record chunks
  registerDoParallel()
  outputPar <- foreach(i = 1:length(split_queries_values), 
                       .packages = c("DBI", "odbc")) %dopar% {
                         con_prod <- dbConnect(odbc(), prod_dsn)
                         tryCatch({
                           dbBegin(con_prod)
                           dbExecute(con_prod, split_queries_values[[i]])
                           dbCommit(con_prod)
                           dbDisconnect(con_prod)
                         },
                         error = function(err){
                           print("error")
                           dbRollback(con_prod)
                           dbDisconnect(con_prod)
                         })
                       }
  registerDoSEQ()
  cat(paste(mshq_inserts_diff, "has been added to", mshq_destination_schema))
} else {
  cat(paste(mshq_destination_schema, "is already up to date"))
}

```

### Update BISLR Data Tables
```{r Update BISLR Oracle Data Tables , echo=FALSE}
# establish connection to OAO Development
con_prod <- dbConnect(odbc(), prod_dsn)
con_dev <- dbConnect(odbc(), dev_dsn)

# establish destination table
bislr_destination_table <- "LPM_DATA_BISLR_ORACLE"
bislr_destination_schema <- "OAO_PRODUCTION"

# identify what files are present in dev schema and not production
bislr_inserts_diff <- setdiff(tbl(con_dev, "LPM_DATA_BISLR_ORACLE") %>%
                          select(FILE_NAME) %>%
                          distinct() %>%
                          collect() %>%
                          pull(),
                        tbl(con_prod, "LPM_DATA_BISLR_ORACLE") %>%
                          select(FILE_NAME) %>%
                          distinct() %>%
                          collect() %>%
                          pull())

if (length(bislr_inserts_diff) > 0) {
  # prepare the insert df based on files not present in prod
  bislr_insert_df <- tbl(con_dev, "LPM_DATA_BISLR_ORACLE") %>%
    filter(FILE_NAME %in% bislr_inserts_diff) %>%
    collect() %>%
    mutate(POSITION_CODE = replace_na(POSITION_CODE, ""),
           POSITION_CODE_DESCRIPTION = replace_na(POSITION_CODE_DESCRIPTION, ""),
           LOCATION_DESCRIPTION = replace_na(LOCATION_DESCRIPTION, ""),
           REVERSE_MAP_HOME = replace_na(REVERSE_MAP_HOME, ""),
           REVERSE_MAP_WORKED = replace_na(REVERSE_MAP_WORKED, ""),
           HOME_DEPARTMENT_NAME = gsub("\'", "''", HOME_DEPARTMENT_NAME),
           WORKED_DEPARTMENT_NAME = gsub("\'", "''", WORKED_DEPARTMENT_NAME),
           POSITION_CODE_DESCRIPTION = gsub("\'", "''", POSITION_CODE_DESCRIPTION),
           EMPLOYEE_NAME = gsub("\'", "''", EMPLOYEE_NAME),
           LOCATION_DESCRIPTION = gsub("\'", "''", LOCATION_DESCRIPTION)) %>%
    mutate(across(everything(), as.character))
  
  # convert the each record/row of tibble to INTO clause of insert statment
  inserts <- 
    lapply(
      lapply(
        lapply(split(bislr_insert_df, 
                     1:nrow(bislr_insert_df)),
               as.list),
        as.character),
      FUN = get_values, bislr_destination_schema, bislr_destination_table)
  
  # create batches of inserts for insert statements
  chunk_length <- 250
  split_queries <- split(inserts, ceiling(seq_along(inserts)/chunk_length))
  
  # append each batch of inserts to batch insert list
  split_queries_values <- list()
  for (i in 1:length(split_queries)) {
    row <- glue_collapse(split_queries[[i]], sep = "\n\n")
    values <- glue('INSERT ALL
                 {row}
                 SELECT 1 from DUAL;')
    split_queries_values <- append(split_queries_values, values)
  }
  
  # execute parallel inserts of 250 record chunks
  registerDoParallel()
  outputPar <- foreach(i = 1:length(split_queries_values), 
                       .packages = c("DBI", "odbc")) %dopar% {
                         con_prod <- dbConnect(odbc(), prod_dsn)
                         tryCatch({
                           dbBegin(con_prod)
                           dbExecute(con_prod, split_queries_values[[i]])
                           dbCommit(con_prod)
                           dbDisconnect(con_prod)
                         },
                         error = function(err){
                           print("error")
                           dbRollback(con_prod)
                           dbDisconnect(con_prod)
                         })
                       }
  registerDoSEQ()
  cat(paste(bislr_inserts_diff, "has been added to", bislr_destination_schema))
} else {
  cat(paste(bislr_destination_schema, "is already up to date"))
}

```